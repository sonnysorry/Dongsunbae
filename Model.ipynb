{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_lstm_cell(mode, hiddenSize, index):\n",
    "    cell = tf.nn.rnn_cell.BasicLSTMCell(hiddenSize, name = \"lstm\"+str(index))\n",
    "    if mode = tf.estimator.ModeKeys.TRAIN:\n",
    "        cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=DEFINES.dropout_width)\n",
    "    return cell\n",
    "\n",
    "def model(features, labels, mode, params):\n",
    "    TRAIN = mode = tf.estimator.ModeKeys.TRAIN\n",
    "    EVAL = mode = tf.estimator.ModeKeys.EVAL\n",
    "    PREDICT = mode = tf.estimator.ModeKeys.PREDICT\n",
    "    \n",
    "    if params['embedding'] = True:\n",
    "        initializer = tf.contrib.layers.xavier_initializer()\n",
    "        embedding_encoder = tf.get+variable(name = \"embedding_encoder\",\n",
    "                                           shape = [params['vocabulary_length'],\n",
    "                                                   params['embedding_size']],\n",
    "                                           dtype=tf.flat32,\n",
    "                                           initializer=initializer,\n",
    "                                           trainable=True)\n",
    "    else:\n",
    "        embedding_encoder = tf.eye(num_rows = params['vocabulary_length'],\n",
    "                                  dtype = tf.float32)\n",
    "        embeddding_encoder = tf,get_variable(name = \"embedding_encoder\",\n",
    "                                            initialer = embedding_encoder,\n",
    "                                            trainable = False)\n",
    "        \n",
    "        embedding_encoder_batch = tf,nn.embedding_lookup(params = embedding_encoder,\n",
    "                                                        ids = features['input'])\n",
    "        \n",
    "    if params['embedding'] = True:\n",
    "        initializer = tf.contrib.layers.xavier_initializer()\n",
    "        embedding_decoder = tf.get_variable(name = \"embedding_decoder\",\n",
    "                                           shape=[params['vocabulary_length'],\n",
    "                                            params['embedding_size']],\n",
    "                                           dtype=tf.float32,\n",
    "                                           initialzer=initializer,\n",
    "                                            trainable=True)\n",
    "    else:\n",
    "        embedding_decoder = tf.eye(num_rows = params['vocabulary_length'],\n",
    "                                  dtype = tf.float32)\n",
    "        embedding_decoder = tf.get_variable(name = 'embeddding_decoder',\n",
    "                                           initialer = embedding_decoder,\n",
    "                                           trainable = False)\n",
    "    \n",
    "    embeddding_decoder_batch = tf.nn.embedding_lookup(params = embedding_decoder,\n",
    "                                                     ids = features['output'])\n",
    "    \n",
    "    with tf.variable_scape('encoder_scope', reuse=tf.AUTO_REUSE):\n",
    "        if params['multilayer'] = True:\n",
    "            encoder_cell_list = [make_lstm_cell(mode, params['hidden_size'],i)\n",
    "                                for i in range(params['layer_size'])]\n",
    "            rnn_cell = tf.contrib.rnn.MultiRNNCell(encoder_cell_list)\n",
    "        else:\n",
    "            rnn_cell = make_lstm_cell(mode, params['hidden_size'], \"\")\n",
    "        encoder_outputs, encoder_states = tf.nn.dynamic_rnn(cell=rnn_cell,\n",
    "                                                           inputs=embedding_encoder_batch,\n",
    "                                                           dtype=tf.float32)\n",
    "        \n",
    "    with tf.variable_scope('decoder_scope', reuse=tf.AUTO_REUSE):\n",
    "        if params['multilayer'] = True:\n",
    "            decoder_cell_list = [make_lstm_cell(mode, params['hidden_size'],i)\n",
    "                                for i in range(params['layer_size'])]\n",
    "            rnn_cell = tf.contrib.rnn.MultiRNNCell(decoder_cell_list)\n",
    "        else:\n",
    "            rnn_cell = make_lstm_cell(mode, params['hidden_size'], \"\")\n",
    "        \n",
    "        decoder_initial_state = encoder_states\n",
    "        decoder_outputs, decoder_states = tf.nn.dynamic_rnn(cell=rnn_cell,\n",
    "                                                           inputs=embedding_decoder_batch,\n",
    "                                                           initial_state=decoder_initial_stae,\n",
    "                                                           dtype=tf.float32)\n",
    "        \n",
    "        logits = tf.keras.layers.Dense(params['vocabulary_length'])(decoder_outputs)\n",
    "        \n",
    "        predict = tf.argmax(logits, 2)\n",
    "        \n",
    "        if PREDICT:\n",
    "            predictions = {\n",
    "                'indexs': predict,\n",
    "                'logits': logits,\n",
    "            }\n",
    "            return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
    "        \n",
    "        labels_ = tf.one_hot(labels, params['vocabulary_length'])\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2\n",
    "                             (logits=logits, labels=labels_))\n",
    "        accuracy = tf.metrics.accuracy(labels=labels,\n",
    "                                      predictions=predict,name='acc0p')\n",
    "        \n",
    "        metrics = {'accuracy': accuracy}\n",
    "        tf.summary.scalar('accuracy', accuracy[1])\n",
    "        \n",
    "        if EVAL:\n",
    "            return tf.estimator.EstimatorSpec(mode,\n",
    "                                             loss=loss,\n",
    "                                              eval_metric_ops=metrics)\n",
    "        assert TRAIN\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=DEFINES.learning_rate)\n",
    "        train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\n",
    "        return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
